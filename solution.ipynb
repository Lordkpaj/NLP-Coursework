{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266d3ac8",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "57b6969b",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar  4 07:39:36 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P4000        Off  | 00000000:00:05.0 Off |                  N/A |\n",
      "| 52%   45C    P0    28W / 105W |      0MiB /  8119MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check which gpu we're using\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d915c684",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6be4d541",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.17.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2021.10.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (1.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.8/site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (7.6.5)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.28.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.5.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.6)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (58.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.0.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.2.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.11.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pandas\n",
    "\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e17a5833",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "63db2fd6",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
    "    DEVICE = 'cpu'\n",
    "else:\n",
    "    print(\"CUDA is available\")\n",
    "    DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7662b4",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "93321177",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c495c0",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 4,
     "id": "a0090edb",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dont_patronize_me import DontPatronizeMe\n",
    "dpm = DontPatronizeMe('./data', './data/TEST/task4_test.tsv')\n",
    "dpm.load_task1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343271a1",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "1077633e",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@@24942188</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ph</td>\n",
       "      <td>We 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@@21968160</td>\n",
       "      <td>migrant</td>\n",
       "      <td>gh</td>\n",
       "      <td>In Libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@@16584954</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"White House press secretary Sean Spicer said ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@@7811231</td>\n",
       "      <td>disabled</td>\n",
       "      <td>nz</td>\n",
       "      <td>Council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@@1494111</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ca</td>\n",
       "      <td>\"\"\" Just like we received migrants fleeing El ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10464</th>\n",
       "      <td>10465</td>\n",
       "      <td>@@14297363</td>\n",
       "      <td>women</td>\n",
       "      <td>lk</td>\n",
       "      <td>\"Sri Lankan norms and culture inhibit women fr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10465</th>\n",
       "      <td>10466</td>\n",
       "      <td>@@70091353</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ph</td>\n",
       "      <td>He added that the AFP will continue to bank on...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10466</th>\n",
       "      <td>10467</td>\n",
       "      <td>@@20282330</td>\n",
       "      <td>in-need</td>\n",
       "      <td>ng</td>\n",
       "      <td>\"\"\" She has one huge platform , and informatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10467</th>\n",
       "      <td>10468</td>\n",
       "      <td>@@16753236</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>in</td>\n",
       "      <td>\"\"\" Anja Ringgren Loven I ca n't find a word t...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10468</th>\n",
       "      <td>10469</td>\n",
       "      <td>@@16779383</td>\n",
       "      <td>homeless</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"\"\" Guinness World Record of 540lbs of 7-layer...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10469 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      par_id      art_id     keyword country  \\\n",
       "0          1  @@24942188    hopeless      ph   \n",
       "1          2  @@21968160     migrant      gh   \n",
       "2          3  @@16584954   immigrant      ie   \n",
       "3          4   @@7811231    disabled      nz   \n",
       "4          5   @@1494111     refugee      ca   \n",
       "...      ...         ...         ...     ...   \n",
       "10464  10465  @@14297363       women      lk   \n",
       "10465  10466  @@70091353  vulnerable      ph   \n",
       "10466  10467  @@20282330     in-need      ng   \n",
       "10467  10468  @@16753236    hopeless      in   \n",
       "10468  10469  @@16779383    homeless      ie   \n",
       "\n",
       "                                                    text  label orig_label  \n",
       "0      We 're living in times of absolute insanity , ...      0          0  \n",
       "1      In Libya today , there are countless number of...      0          0  \n",
       "2      \"White House press secretary Sean Spicer said ...      0          0  \n",
       "3      Council customers only signs would be displaye...      0          0  \n",
       "4      \"\"\" Just like we received migrants fleeing El ...      0          0  \n",
       "...                                                  ...    ...        ...  \n",
       "10464  \"Sri Lankan norms and culture inhibit women fr...      0          1  \n",
       "10465  He added that the AFP will continue to bank on...      0          0  \n",
       "10466  \"\"\" She has one huge platform , and informatio...      1          3  \n",
       "10467  \"\"\" Anja Ringgren Loven I ca n't find a word t...      1          4  \n",
       "10468  \"\"\" Guinness World Record of 540lbs of 7-layer...      1          3  \n",
       "\n",
       "[10469 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpm.train_task1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd02e3",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "257e4b86",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "source": [
    "## Split into train and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e6de66",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 6,
     "id": "c21335f3",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4341</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4136</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10352</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8279</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1164</td>\n",
       "      <td>[1, 0, 0, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  par_id                  label\n",
       "0   4341  [1, 0, 0, 1, 0, 0, 0]\n",
       "1   4136  [0, 1, 0, 0, 0, 0, 0]\n",
       "2  10352  [1, 0, 0, 0, 0, 1, 0]\n",
       "3   8279  [0, 0, 0, 1, 0, 0, 0]\n",
       "4   1164  [1, 0, 0, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get training set and dev set ids\n",
    "practice_splits_dir = './data/practice_splits/'\n",
    "train_ids = pd.read_csv(practice_splits_dir + 'train_semeval_parids-labels.csv')\n",
    "dev_ids = pd.read_csv(practice_splits_dir + 'dev_semeval_parids-labels.csv')\n",
    "# convert ids to strings\n",
    "train_ids.par_id = train_ids.par_id.astype(str)\n",
    "dev_ids.par_id = dev_ids.par_id.astype(str)\n",
    "train_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886574fe",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 7,
     "id": "625450f3",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [],
   "source": [
    "def extract_split_data(ids_df, original_df):\n",
    "    \"\"\" ids_df is dataframe with columns 'par_id', 'label'\n",
    "        original_df is original dataframe with columns 'par_id', 'text', 'label', etc.\n",
    "    \"\"\"\n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(ids_df)):  \n",
    "        par_id = ids_df.par_id[idx]\n",
    "        # select row from original dataset to retrieve `text` and binary label\n",
    "        text = original_df.loc[original_df.par_id == par_id].text.values[0]\n",
    "        label = original_df.loc[original_df.par_id == par_id].label.values[0]\n",
    "        rows.append({\n",
    "            'par_id':par_id,\n",
    "            'text':text,\n",
    "            'label':label\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b8d9f0c",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 8,
     "id": "0cf74986",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4341</td>\n",
       "      <td>The scheme saw an estimated 150,000 children f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4136</td>\n",
       "      <td>Durban 's homeless communities reconciliation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10352</td>\n",
       "      <td>The next immediate problem that cropped up was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8279</td>\n",
       "      <td>Far more important than the implications for t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1164</td>\n",
       "      <td>To strengthen child-sensitive social protectio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8370</th>\n",
       "      <td>8380</td>\n",
       "      <td>Rescue teams search for survivors on the rubbl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8371</th>\n",
       "      <td>8381</td>\n",
       "      <td>The launch of ' Happy Birthday ' took place la...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>8382</td>\n",
       "      <td>The unrest has left at least 20,000 people dea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>8383</td>\n",
       "      <td>You have to see it from my perspective . I may...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>8384</td>\n",
       "      <td>Yet there was one occasion when we went to the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8375 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     par_id                                               text  label\n",
       "0      4341  The scheme saw an estimated 150,000 children f...      1\n",
       "1      4136  Durban 's homeless communities reconciliation ...      1\n",
       "2     10352  The next immediate problem that cropped up was...      1\n",
       "3      8279  Far more important than the implications for t...      1\n",
       "4      1164  To strengthen child-sensitive social protectio...      1\n",
       "...     ...                                                ...    ...\n",
       "8370   8380  Rescue teams search for survivors on the rubbl...      0\n",
       "8371   8381  The launch of ' Happy Birthday ' took place la...      0\n",
       "8372   8382  The unrest has left at least 20,000 people dea...      0\n",
       "8373   8383  You have to see it from my perspective . I may...      0\n",
       "8374   8384  Yet there was one occasion when we went to the...      0\n",
       "\n",
       "[8375 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = extract_split_data(train_ids, dpm.train_task1_df)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b723b5cf",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 9,
     "id": "4920a4d3",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4046</td>\n",
       "      <td>We also know that they can benefit by receivin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1279</td>\n",
       "      <td>Pope Francis washed and kissed the feet of Mus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8330</td>\n",
       "      <td>Many refugees do n't want to be resettled anyw...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4063</td>\n",
       "      <td>\"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4089</td>\n",
       "      <td>\"In a 90-degree view of his constituency , one...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>10462</td>\n",
       "      <td>The sad spectacle , which occurred on Saturday...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>10463</td>\n",
       "      <td>\"\"\" The Pakistani police came to our house and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>10464</td>\n",
       "      <td>\"When Marie O'Donoghue went looking for a spec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>10465</td>\n",
       "      <td>\"Sri Lankan norms and culture inhibit women fr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>10466</td>\n",
       "      <td>He added that the AFP will continue to bank on...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2094 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     par_id                                               text  label\n",
       "0      4046  We also know that they can benefit by receivin...      1\n",
       "1      1279  Pope Francis washed and kissed the feet of Mus...      1\n",
       "2      8330  Many refugees do n't want to be resettled anyw...      1\n",
       "3      4063  \"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...      1\n",
       "4      4089  \"In a 90-degree view of his constituency , one...      1\n",
       "...     ...                                                ...    ...\n",
       "2089  10462  The sad spectacle , which occurred on Saturday...      0\n",
       "2090  10463  \"\"\" The Pakistani police came to our house and...      0\n",
       "2091  10464  \"When Marie O'Donoghue went looking for a spec...      0\n",
       "2092  10465  \"Sri Lankan norms and culture inhibit women fr...      0\n",
       "2093  10466  He added that the AFP will continue to bank on...      0\n",
       "\n",
       "[2094 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set = extract_split_data(dev_ids, dpm.train_task1_df)\n",
    "dev_set_short = extract_split_data(dev_ids[150:250].reset_index(drop=True), dpm.train_task1_df)\n",
    "dev_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38c9b6",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "2ce0a533",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "source": [
    "## Define downsampling/upsampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b1d836",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 10,
     "id": "0bfaa5ff",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [],
   "source": [
    "def downsample(train_set, ratio=2):\n",
    "    \"\"\" Downsample (majority) negative instances, so num_negative is ratio * num_positive\n",
    "        args:\n",
    "            ratio: The ratio of negative (majority) samples compared to positive (minority) samples\n",
    "    \"\"\"\n",
    "    \n",
    "    pos_samples = train_set[train_set.label==1]\n",
    "    neg_samples = train_set[train_set.label==0]\n",
    "\n",
    "    print(\"Number of positive samples:\", len(pos_samples))\n",
    "    print(\"Number of negative samples:\", len(neg_samples))\n",
    "\n",
    "    res = pd.concat([pos_samples, neg_samples[:len(pos_samples)*ratio]])\n",
    "\n",
    "    print(\"Number of negative samples after downsampling:\", len(res[res.label==0]))\n",
    "\n",
    "    return res\n",
    "\n",
    "def upsample(train_set, ratio=2):\n",
    "    \"\"\" Upsample (minority) positive instances, so num_negative is ratio * num_positive\n",
    "        args:\n",
    "            ratio: The ratio of negative (majority) samples compared to positive (minority) samples\n",
    "    \"\"\"\n",
    "    \n",
    "    pos_samples = train_set[train_set.label==1]\n",
    "    neg_samples = train_set[train_set.label==0]\n",
    "\n",
    "    print(\"Number of positive samples:\", len(pos_samples))\n",
    "    print(\"Number of negative samples:\", len(neg_samples))\n",
    "\n",
    "    res = pd.concat([pos_samples.sample(len(neg_samples)//ratio, replace=True), neg_samples])\n",
    "\n",
    "    print(\"Number of positive samples after upsampling:\", len(res[res.label==1]))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7fb4b",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "71a67059",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "source": [
    "## Define Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c97ced",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 13,
     "id": "e89fb791",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PatroniseDataset(Dataset):\n",
    "    def __init__(self, tokenizer, input_set):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = input_set['text']\n",
    "        self.labels = input_set['label']\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        texts = []\n",
    "        labels = []\n",
    "\n",
    "        for b in batch:\n",
    "            texts.append(b['text'])\n",
    "            labels.append(b['label'])\n",
    "\n",
    "        # The maximum sequence size for BERT is 512 but here the tokenizer truncate sentences longer than 128 tokens.  \n",
    "        # We also pad shorter sentences to a length of 128 tokens\n",
    "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        encodings['labels'] =  torch.tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        return encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError\n",
    "        item = {'text': self.texts[idx],\n",
    "                'label': self.labels[idx]}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec936af8",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b5489d",
   "metadata": {},
   "source": [
    "Models:\n",
    "- BERT\n",
    "- RoBERTa\n",
    "- XLNet\n",
    "- DeBERTa\n",
    "\n",
    "For each, change:\n",
    "- learning_rate\n",
    "- batch_size\n",
    "- num_epochs\n",
    "- downsampling/upsampling\n",
    "- loss weighting of pos/neg samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ea467",
   "metadata": {},
   "source": [
    "## AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf3076",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861060e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8009759",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d5f5c1c6",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "source": [
    "### Define Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd0ea6a",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 21,
     "id": "24aa617a",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [],
   "source": [
    "def predict_patronise(inputs, tokenizer, model): \n",
    "    model.eval()\n",
    "    encodings = tokenizer(inputs, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    encodings.to(DEVICE)\n",
    "    output = model(**encodings)\n",
    "    preds = torch.argmax(output.logits, axis=-1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15b3363e",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 22,
     "id": "da2f6334",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, data_loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader):\n",
    "            labels = data['label']\n",
    "            texts = data['text']\n",
    "            pred = predict_patronise(texts, tokenizer, model)\n",
    "            all_preds += pred.tolist()\n",
    "            all_labels += labels.tolist()\n",
    "\n",
    "    # with the saved predictions and labels we can compute accuracy, precision, recall and f1-score\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Not patronising\", \"Patronising\"], output_dict=True)\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "194e95f1",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 25,
     "id": "e52806b0",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [],
   "source": [
    "def display_report(report):\n",
    "    print(\"Not patronising:\")\n",
    "    for k, v in report['Not patronising'].items():\n",
    "        print(f\"{k:<10}: {v}\")\n",
    "\n",
    "    print(\"\\nPatronising:\")\n",
    "    for k, v in report['Patronising'].items():\n",
    "        print(f\"{k:<10}: {v}\")\n",
    "\n",
    "    print(\"\\nAccuracy:\", report['accuracy'])\n",
    "    print(\"\\nThe f1-score we care about:\", report['Patronising']['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d5a3a",
   "metadata": {},
   "source": [
    "### Define overall pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6062390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "results_filename = \"./results\"\n",
    "\n",
    "def load_results():\n",
    "    if not exists(results_filename):\n",
    "        return pd.DataFrame()\n",
    "    results = pd.read_csv(results_filename)\n",
    "    results['learning_rate'] = results['learning_rate'].astype(float)\n",
    "    results['batch_size'] = results['batch_size'].astype(int)\n",
    "    results['num_epochs'] = results['num_epochs'].astype(int)\n",
    "    results['sampling'] = results['sampling'].astype(int)\n",
    "    results['sampling_ratio'] = results['sampling_ratio'].astype(int)\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_results(results):\n",
    "    results.to_csv(results_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f277b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_folds(train_set, n=5):\n",
    "    np.random.seed(0)\n",
    "    perm = np.random.permutation(len(train_set))\n",
    "    indexes = np.arange(len(train_set))\n",
    "    indexes = indexes[perm]\n",
    "    folds = np.array_split(indexes, n)\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f6ba64f",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 19,
     "id": "9eb1ac5f",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [],
   "source": [
    "def main_patronise_cross_val(pretrained_model_name='bert-base-cased', learning_rate=0.0001, batch_size=32, num_epochs=3, sampling=1, sampling_ratio=2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pretrained_model_name\n",
    "        learning_rate\n",
    "        batch_size\n",
    "        num_epochs\n",
    "        sampling: (0 = use original data) (1 = downsample majority class) (2 = upsample minority class)\n",
    "        sampling_ratio: the ratio of negative (majority) samples compared to positive (minority) samples\n",
    "    \"\"\"\n",
    "    \n",
    "    # create tokenizer for specified pretrained model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "    \n",
    "    # upsample / downsample\n",
    "    train_set_sampled = downsample(train_set) if sampling == 1 else upsample(train_set) if sampling == 2 else train_set\n",
    "    \n",
    "    folds = get_folds(train_set_sampled, n=5)\n",
    "    reports = []\n",
    "    for fold in folds:\n",
    "        # split into train and val for this fold\n",
    "        val_indexes = fold\n",
    "        train_indexes = list(set(range(train_set_sampled.shape[0])) - set(val_indexes))\n",
    "        val_set_i = train_set_sampled.iloc[val_indexes].reset_index()\n",
    "        train_set_i = train_set_sampled.iloc[train_indexes].reset_index()\n",
    "        \n",
    "        # create dataset\n",
    "        train_dataset = PatroniseDataset(tokenizer, train_set_i)\n",
    "\n",
    "        # create classification model with specified pretrained model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=2)\n",
    "\n",
    "        # train model\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir = './experiment/patronise',\n",
    "            learning_rate = learning_rate,\n",
    "            logging_steps = 100,\n",
    "            per_device_train_batch_size = batch_size,\n",
    "            num_train_epochs = num_epochs,\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model = model,                         \n",
    "            args = training_args,                 \n",
    "            train_dataset = train_dataset,                   \n",
    "            data_collator = train_dataset.collate_fn\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "        # evaluate model on this fold's val set\n",
    "        val_dataset = PatroniseDataset(tokenizer, val_set_i)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "        report = evaluate(model, tokenizer, val_loader)\n",
    "        reports.append(report)\n",
    "    \n",
    "    avg_report = {}\n",
    "    for lbl in ['Not patronising', 'Patronising']:\n",
    "        avg_report[lbl] = {}\n",
    "        for k in reports[0][lbl].keys():\n",
    "            avg_report[lbl][k] = sum([report[lbl][k] for report in reports]) / len(folds)\n",
    "    avg_report['accuracy'] = sum([report['accuracy'] for report in reports]) / len(folds)\n",
    "    \n",
    "    return avg_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b82ac",
   "metadata": {},
   "source": [
    "### Base-model-choosing and Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c3fe0",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 20,
     "id": "57624943",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [],
   "source": [
    "model_names = ['bert-base-cased', 'roberta-base', 'deberta-base']\n",
    "learning_rates = [0.0001, 0.0005, 0.001]\n",
    "batch_sizes = [16, 32, 64]\n",
    "num_epochses = [1, 3, 5]\n",
    "samplings = [(0,0), (1,2), (2,2)] # [(fst,snd)] where fst is no sampling (0), downsampling (1) or upsampling (2); snd is sampling ratio\n",
    "\n",
    "# load results from previous experiments\n",
    "reports = load_results()\n",
    "\n",
    "for pretrained_model_name in model_names:\n",
    "    for learning_rate in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            for num_epochs in num_epochses:\n",
    "                for (sampling, sampling_ratio) in samplings:\n",
    "                    # if this configuration has already been done, skip it\n",
    "                    already_exists = not reports.empty and not reports.loc[(reports['pretrained_model_name']==pretrained_model_name) &\n",
    "                                                                   (reports['learning_rate']==learning_rate) &\n",
    "                                                                   (reports['batch_size']==batch_size) &\n",
    "                                                                   (reports['num_epochs']==num_epochs) &\n",
    "                                                                   (reports['sampling']==sampling) &\n",
    "                                                                   (reports['sampling_ratio']==sampling_ratio)].empty\n",
    "                    if already_exists:\n",
    "                        continue\n",
    "                        \n",
    "                    # perform cross validation with this base model and hyperparameter configuration\n",
    "                    report = main_patronise_cross_val(pretrained_model_name=pretrained_model_name,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            batch_size=batch_size,\n",
    "                                            num_epochs=num_epochs,\n",
    "                                            sampling=sampling,\n",
    "                                            sampling_ratio=sampling_ratio)\n",
    "                    \n",
    "                    # add configuration and metrics to results dataframe; save results\n",
    "                    row = {'pretrained_model_name': pretrained_model_name,\n",
    "                           'learning_rate': learning_rate,\n",
    "                           'batch_size': batch_size,\n",
    "                           'num_epochs': num_epochs,\n",
    "                           'sampling': sampling,\n",
    "                           'sampling_ratio': sampling_ratio}\n",
    "                    print(row)\n",
    "                    display_report(report)\n",
    "                    for k, v in report['Not patronising'].items():\n",
    "                        row[f\"neg_{k}\"] = v\n",
    "                    for k, v in report['Patronising'].items():\n",
    "                        row[f\"pos_{k}\"] = v\n",
    "                    row['accuracy'] = report['accuracy']\n",
    "                    reports = reports.append(row, ignore_index=True)\n",
    "                    save_results(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe947e4",
   "metadata": {},
   "source": [
    "### Train with best model and optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed2f0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_filename(model_name, learning_rate, batch_size, num_epochs, sampling, sampling_ratio):\n",
    "    sampling_text = f\"{'downsampling' if sampling == 1 else 'upsampling'}_{sampling_ratio}\" if sampling else \"\"\n",
    "    model_filename = f\"./models/patronise_{model_name}_{learning_rate}_{batch_size}_{num_epochs}_{sampling_text}/\"\n",
    "    return model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fb06557",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 19,
     "id": "9eb1ac5f",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [],
   "source": [
    "def train_final_model(pretrained_model_name='bert-base-cased', learning_rate=0.0001, batch_size=32, num_epochs=3, sampling=1, sampling_ratio=2, save_model=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pretrained_model_name\n",
    "        learning_rate\n",
    "        batch_size\n",
    "        num_epochs\n",
    "        sampling: (0 = use original data) (1 = downsample majority class) (2 = upsample minority class)\n",
    "        sampling_ratio: the ratio of negative (majority) samples compared to positive (minority) samples\n",
    "        save_model: boolean value indicating whether or not to save model parameters/weights\n",
    "    \"\"\"\n",
    "    # Form model filename\n",
    "    model_filename = generate_model_filename(pretrained_model_name, learning_rate, batch_size, num_epochs, sampling, sampling_ratio)\n",
    "    \n",
    "    if exists(model_filename):\n",
    "        print(\"Model already exists\")\n",
    "        return model_filename\n",
    "    \n",
    "    # create tokenizer for specified pretrained model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "    \n",
    "    # upsample / downsample\n",
    "    train_set_sampled = downsample(train_set) if sampling == 1 else upsample(train_set) if sampling == 2 else train_set\n",
    "    \n",
    "    # create dataset\n",
    "    train_dataset = PatroniseDataset(tokenizer, train_set_sampled)\n",
    "    \n",
    "    # create classification model with specified pretrained model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=2)\n",
    "    \n",
    "    # train model\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = './experiment/patronise',\n",
    "        learning_rate = learning_rate,\n",
    "        logging_steps = 100,\n",
    "        per_device_train_batch_size = batch_size,\n",
    "        num_train_epochs = num_epochs,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model = model,                         \n",
    "        args = training_args,                 \n",
    "        train_dataset = train_dataset,                   \n",
    "        data_collator = train_dataset.collate_fn\n",
    "    )\n",
    "    trainer.train()\n",
    "    \n",
    "    trainer.save_model(model_filename)\n",
    "    return model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7e7116e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best base model: roberta-base\n",
      "\n",
      "Optimal hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "batch_size: 32\n",
      "num_epochs: 3\n",
      "sampling: 1\n",
      "sampling_ratio: 2\n"
     ]
    }
   ],
   "source": [
    "# Get optimal hyperparameters\n",
    "reports = load_results()\n",
    "optimal_hyperparams = reports.iloc[[reports['pos_f1-score'].idxmax()]]\n",
    "\n",
    "print(\"Best base model:\", optimal_hyperparams['pretrained_model_name'].item())\n",
    "print(\"\\nOptimal hyperparameters:\")\n",
    "print(\"learning_rate:\", optimal_hyperparams['learning_rate'].item())\n",
    "print(\"batch_size:\", optimal_hyperparams['batch_size'].item())\n",
    "print(\"num_epochs:\", optimal_hyperparams['num_epochs'].item())\n",
    "print(\"sampling:\", optimal_hyperparams['sampling'].item())\n",
    "print(\"sampling_ratio:\", optimal_hyperparams['sampling_ratio'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a5e5fe0",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 23,
     "id": "1401a6d7",
     "kernelId": "308f8cd4-86c5-47d4-baa3-e60c262101a4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples: 794\n",
      "Number of negative samples: 7581\n",
      "Number of negative samples after downsampling: 1588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2382\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 225\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 02:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.571100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.398100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./models/patronise_roberta-base_0.0001_32_3_downsampling_2/\n",
      "Configuration saved in ./models/patronise_roberta-base_0.0001_32_3_downsampling_2/config.json\n",
      "Model weights saved in ./models/patronise_roberta-base_0.0001_32_3_downsampling_2/pytorch_model.bin\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file ./models/patronise_roberta-base_0.0001_32_3_downsampling_2/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/patronise_roberta-base_0.0001_32_3_downsampling_2/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./models/patronise_roberta-base_0.0001_32_3_downsampling_2/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./models/patronise_roberta-base_0.0001_32_3_downsampling_2/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Train final model\n",
    "final_model_filename = train_final_model(pretrained_model_name=optimal_hyperparams['pretrained_model_name'].values[0],\n",
    "                                         learning_rate=optimal_hyperparams['learning_rate'].item(),\n",
    "                                         batch_size=optimal_hyperparams['batch_size'].item(),\n",
    "                                         num_epochs=optimal_hyperparams['num_epochs'].item(),\n",
    "                                         sampling=optimal_hyperparams['sampling'].item(),\n",
    "                                         sampling_ratio=optimal_hyperparams['sampling_ratio'].item())\n",
    "\n",
    "final_tokenizer = AutoTokenizer.from_pretrained(optimal_hyperparams['pretrained_model_name'].values[0])\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(final_model_filename, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af1744db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 262/262 [00:13<00:00, 18.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not patronising:\n",
      "precision : 0.9662921348314607\n",
      "recall    : 0.8622691292875989\n",
      "f1-score  : 0.9113218070273285\n",
      "support   : 1895\n",
      "\n",
      "Patronising:\n",
      "precision : 0.3523573200992556\n",
      "recall    : 0.7135678391959799\n",
      "f1-score  : 0.4717607973421927\n",
      "support   : 199\n",
      "\n",
      "Accuracy: 0.8481375358166189\n",
      "\n",
      "The f1-score we care about: 0.4717607973421927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on dev set\n",
    "final_model.to(DEVICE)\n",
    "dev_dataset = PatroniseDataset(final_tokenizer, dev_set)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=8)\n",
    "dev_report = evaluate(final_model, final_tokenizer, dev_loader)\n",
    "\n",
    "display_report(dev_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50540089",
   "metadata": {},
   "source": [
    "### Predict on held out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44076fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "dpm.load_test()\n",
    "dpm.test_set_df\n",
    "\n",
    "# Drop unneeded columns\n",
    "dpm.test_set_df.drop(columns=['art_id', 'keyword', 'country'], inplace=True)\n",
    "dpm.test_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to save predictions to an output file\n",
    "def save_predictions_to_file(preds, filename):\n",
    "    with open(filename,'w') as f:\n",
    "        for pred in preds:\n",
    "            f.write(str(pred.item())+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "test_preds = []\n",
    "test_batch_size = 32\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(dpm.test_set_df), test_batch_size)):\n",
    "        final_model.eval()\n",
    "        encodings = final_tokenizer([str(x) for x in dpm.test_set_df['text'][i:i+test_batch_size]], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        encodings.to(DEVICE)\n",
    "        output = final_model(**encodings)\n",
    "        test_preds += torch.argmax(output.logits, axis=-1)\n",
    "\n",
    "test_preds_filename = 'final_model_test_preds.txt'\n",
    "save_predictions_to_file(test_preds, test_preds_filename)\n",
    "print(\"Test predictions saved to file:\", test_preds_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7921b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b223488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c8978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
